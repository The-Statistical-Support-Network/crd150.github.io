---
title: "Coordinate Reference Systems and Geocoding"
subtitle: <h4 style="font-style:normal">CRD 150 - Quantitative Methods in Community Research</h4>
author: <h4 style="font-style:normal">Professor Noli Brazil</h4>
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: cosmo
    code_folding: show
---


<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: italic;
}

h1.title {
  font-weight: bold;
}

</style>
\

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


In this guide we will cover the spatial data wrangling task of converting point data from a nonspatial to a spatial format.  This process involves using geographic coordinates (longitude and latitude) to place points on a map.  In some cases, you don't have geographic coordinates but street addresses.  Here, you'll need to geocode your data, which involves converting street addresses to geographic coordinates.  These tasks are intimately related to the concept of projection and reprojection, and underlying all of these concepts is a Coordinate Reference System.  These topics will be covered using Los Angeles City as our case study.

<div style="margin-bottom:25px;">
</div>
## **Setting up the data**
\

We'll need to load required packages.  You should have already installed these packages in prior labs, so no need to run the `install.packages()` function.

```{r message = FALSE, warning = FALSE}
library(sf)
library(tidyverse)
library(sp)
library(tmap)
```

Next, we'll need to bring in shapefiles for Los Angeles City boundaries.  I uploaded a zip file containing Los Angeles City shapefile boundaries on GitHub. There are actually two boundary files - one from the package **tigris** and the other downloaded from the [National Historical Geographic Information System](https://crd150.github.io/nhgis.html#download_shapefiles) (NHGIS). Set your working directory to a folder you want to work out of.  Download the file and unzip it using the following code.

```{r warning = FALSE, message = FALSE, eval = FALSE}
setwd("insert your pathway here")
download.file(url = "https://raw.githubusercontent.com/crd150/data/master/georeferencing_minilab.zip", destfile = "georeferencing_minilab.zip")
unzip(zipfile = "georeferencing_minilab.zip")
```

```{r warning = FALSE, message = FALSE, include = FALSE}
download.file(url = "https://raw.githubusercontent.com/crd150/data/master/georeferencing_minilab.zip", destfile = "georeferencing_minilab.zip")
unzip(zipfile = "georeferencing_minilab.zip")
```

Bring in the **tigris** and NHGIS boundary files 

```{r warning = FALSE, message = FALSE, results = "hide"}
la.city.tigris <- st_read("lacity_tigris.shp")
la.city.nhgis <- st_read("lacity_nhgis.shp")
```

<div style="margin-bottom:25px;">
</div>
## **Bringing in point data: Longitude/Latitude**
\

Many point data sets are not in spatial form (i.e. data you can immediately map).  Best case scenario is that you have a point data set with geographic coordinates.  Geographic coordinates are in the form of a longitude and latitude, where longitude is your X coordinate and spans East/West and latitude is your Y coordinate and spans North/South. 

Let's bring in a csv data set of homeless encampments in Los Angeles City, which was downloaded from the [Los Angeles City Open Data portal](https://data.lacity.org/A-Well-Run-City/MyLA311-Service-Request-Data-2017/d4vt-q4t5) and uploaded onto GitHub.

```{r warning = FALSE, message = FALSE}
homeless311.df <- read_csv("https://raw.githubusercontent.com/crd150/data/master/homeless311_la_2017.csv")
```

The data represent homeless encampment locations as reported through the City's [311 system](https://www.lacity.org/311-services).  We will use the function `st_as_sf()` to create a point **sf** object of *homeless311.df*.  The function requires you to specify the longitude and latitude of each point, which are conveniently located in the variables *longitude* and *latitude* in *homeless311.df*.  The other important option that should be set is `crs=`.  This option specifies the coordinate reference system.  We talked about this in lecture and we'll cover this more a little later, but the most important point is that because we are going to map the points onto the Los Angeles City boundary *la.city.tigris*, we should adopt the CRS of *la.city.tigris*.  The CRS of *la.city.tigris* can be called up by using the `st_crs()` function

```{r}
st_crs(la.city.tigris)
```

We plug this CRS directly into the `st_as_sf()` (along with the longitude and latitude) as follows 

```{r}
homeless311.sf <- st_as_sf(homeless311.df, coords = c("longitude", "latitude"), crs = st_crs(la.city.tigris))
```

Now we can map homeless encampments onto the city's boundaries

```{r message = FALSE, warning = FALSE}
tm_shape(la.city.tigris) +
  tm_polygons() +
tm_shape(homeless311.sf) +  
  tm_dots(col="red")
```  
  
<div style="margin-bottom:25px;">
</div>  
## **Coordinate Reference System**
\

The Coordinate Reference System is an important concept to understand when dealing with spatial data.  We won't go through the *real* nuts and bolts of CRS, which you can read in GWR Chapter 2.4 and 6, but we'll go through enough of it so that you can get through most CRS related spatial data wrangling tasks.

The CRS contains two major components: the Geographic Coordinate System (GCS) and the Projected Coordinate System (PCS).  The GCS can be further separated into two parts: the ellipse and the datum.  The ellipse is a model of the Earth's shape.  The datum defines the coordinate system of this model - the origin point and the axes.  You need these two basic components to place points on Earth's three-dimensional surface.  Think of it as trying to create a globe (ellipse) and figuring out where to place points on that globe (datum).

The PCS then translates these points from a globe onto a two-dimensional space.  We need to do this because were creating maps, not globes (it's kind of hard carrying a globe around when you're finding your way around a city).  

When you call up the CRS of a spatial data set

```{r}
st_crs(la.city.tigris)
```

You see two ways to describe the CRS: an `epsg` code and a `proj4string`.  The `epsg` is a shortcut way for defining all the components of a CRS in one number. You can look up all the possible `epsg` [here](http://spatialreference.org/ref/epsg/).  `proj4string`, in contrast, spells everything out. For some reason, `st_crs()` truncates some of `proj4string`, so let's run the function `proj4string()` on *la.city.tigris*, which we need to convert to an **sp** object using the function `as()` because `proj4string()` does not work on **sf** objects


```{r}
proj4string(as(la.city.tigris, "Spatial"))
```

Here we find the three major components to a CRS.  The GCS ellipse and datum are specified by `ellps` and `datum`, respectively.  The PCS is specified by `+proj`.  


OK, this may be too much info.  Let's step back and ask ourselves why do we need to know about the CRS for practical mapping and analysis purposes? There are three main reasons: 

1. Two objects that are compared or combined should have the same CRS.
2. Aesthetic purposes and/or to correct distortions.
3. Many geometric functions require certain CRSs.

We already talked about reason (1) above when we used the CRS of *la.city.tigris* to define the CRS of *homeless311.sf*.  To illustrate (2), let's compare the **tigris** and NHGIS Los Angeles City boundaries.

We find that *la.city.nhgis* has a different CRS from *la.city.tigris*.  

```{r}
st_crs(la.city.nhgis)
```

Which is not necessarily a problem, but let's map the two boundaries side by side. I used the function `tmap_arrange()` from the **tmap** package to do this

```{r}
lacity1 <- tm_shape(la.city.tigris) +
  tm_polygons() 
lacity2 <- tm_shape(la.city.nhgis) +
  tm_polygons() 

tmap_arrange(lacity1, lacity2)
```

You see a tilt for the NHGIS boundary map on the right.  That tilt is not due to our bad eyesight or anything wrong in our code.  It's due to the projection system that NHGIS uses for their shapefiles, which is Albers Equal Area (`aea`).  Figure 1 shows maps of the United States by different projection systems, include Albers Equal Area (top left)


<center>
![Figure 1: U.S. Projection Coordinate Systems](/Users/noli/Documents/UCD/teaching/CRD150/Lab/crd150.github.io/projections.jpg)

</center>

You'll see that under Albers Equal Area, California tilts.  Hence, Los Angeles will tilt.  In this case, AEA may be better for maps of the entire U.S. or for certain parts of the country (e.g. Midwest).  

To fix the tilt, let's reproject *la.city.nhgis* to the CRS of *la.city.tigris*, which has a PCS of `longlat`.  We use the function `st_transform` to reproject spatial data from one CRS to another.

```{r warning = FALSE, message = FALSE}
la.city.nhgis.tr <-st_transform(la.city.nhgis, st_crs(la.city.tigris)) 
```

Now, no more tilt!

```{r warning = FALSE, message = FALSE}
lacity1 <- tm_shape(la.city.tigris) +
  tm_polygons() 
lacity2 <- tm_shape(la.city.nhgis.tr) +
  tm_polygons() 

tmap_arrange(lacity1, lacity2)
```

Now, what about reason (3) in our list of reasons above?  Right now, the spatial point data of homeless encampments are in longitude/latitude coordinates.  Many of R's geometric functions that require calculating distances (e.g. distance from one point to another) or areas require a standard measure of distance/area.  Distance in latitude/longitude is in decimal degrees, which is not a standard measure.  We can find out the units of a spatial data set by using the `st_crs()` function and calling up units as follows

```{r}
st_crs(homeless311.sf)$units
st_crs(la.city.nhgis.tr)$units
```

Not good. We can reproject the encampments and Los Angeles City boundaries to a CRS that handles standard distance measures such as meters or kilometers.  Going back to Figure 1, a Mercator projection looks appropriate for California.  A [Universal Transverse Mercator](http://desktop.arcgis.com/en/arcmap/latest/map/projections/universal-transverse-mercator.htm) (UTM) coordinate system splits the country into zones so that you can get appropriate (non titling) maps of your specific region.  The zones are shown in Figure 2


<center>
![Figure 2: UTM Zones](/Users/noli/Documents/UCD/teaching/CRD150/Lab/crd150.github.io/utm.png)

</center>

Southern California looks like its in Zone 11.  So, let's reproject *la.city.nhgis* to a UTM Zone 11 projection using `st_transform()`.  We'll have to spell out the PCS (utm in zone 11), datum and ellipse in the `crs=` option.

```{r}
la.city.nhgis.tr2 <-st_transform(la.city.nhgis.tr, crs = "+proj=utm +zone=11 +datum=NAD83 +ellps=GRS80") 
```

Check the units

```{r}
st_crs(la.city.nhgis.tr2)$units
```

"m" means meters.  You can use the same code to reproject *homeless311.sf*.

Another important problem that you may encounter is that a spatial data set you downloaded from a source contains no CRS (unprojected or unknown).  In this case, use the function `st_set_crs()` to set the CRS.  See GWR 6.1 for more details.

Main takeaway points:

1. The CRS for any spatial data set you bring into R should always be established.
1. If you are planning to work with multiple spatial data sets on the same project, make sure they have the same CRS.
2. Make sure the CRS is appropriate for your community.
3. Make sure the CRS is appropriate for the types of spatial tasks you are planning to conduct. 

If you stick with these principles, you should be able to get through most issues regarding CRSs. If you get stuck, read GWR Ch. 2.4 and 6.

<div style="margin-bottom:25px;">
</div>
## **Geocoding addresses**
\

Often you will get point data that won’t have longitude/X and latitude/Y coordinates but instead have street addresses. The process of going from address to X/Y coordinates is known as geocoding.  You can geocode street addresses using the `mutate_geocode()` function in the **ggmap** package. The function will take a data frame with physical addresses and create a new data frame with longitude/X and latitude/Y coordinates for those addresses.  The new data frame will retain all columns of the input data frame. 

`mutate_geocode()` draws from Google Maps’ API to connect street addresses to x,y coordinates. You can think of it as R punching in addresses into Google Maps to find locations like you do when you are trying to find directions.  

You used to be able to use `mutate_geocode()` without an API key, but Google changed that policy on July 2018.  Now, you'll need to register for a Google API key [here](https://developers.google.com/maps/documentation/geocoding/get-api-key).  You'll also need to attach a credit card to this API key. After you sign up for a Google API Key, you will need to enable billing.  To do this, go to https://console.cloud.google.com and select APIs & Services and then Library.  Search for Geocoding API and click on enable.  You'll then want to make sure no one "steals" your API key for nefarious purposes by securing it.  Go to Credentials from your API dashboard, select your key, then select on the API restrictions tab under Key restrictions.  From the pull down menu, select Geocoding API and click save. 

When you sign up for a key, you'll get a \$200 per month credit.  The Geocoding API costs \$5.00 per 1,000 calls, or \$0.005 per call. Over a span of 30 days, the daily usage credit for the Geocoding API is \$6.67 per day. This means that you would be able to make 1,334 calls to the Geocoding API per day for free.  

The R CRAN version of **ggmap** is old and doesn't allow you to register a key.  Download the most recent version of **ggmap** through GitHub using the following code

```{r eval  = FALSE}
if(!requireNamespace("devtools")) install.packages("devtools")
devtools::install_github("dkahle/ggmap", ref = "tidyup")

library(ggmap)
```

```{r include = FALSE}
library(ggmap)
```

If you have an old version of **ggmap** on your hard drive, you may need to restart R after installing **ggmap** to see the new version. Use the function `register_google()` to register your API key. 

```{r eval = FALSE}
register_google(key = 'insert your Google API key here')
```

```{r include = FALSE}
 register_google(key = 'AIzaSyDYCEi_yESAkDn3f2gpeDdIx1Nk0gStGis')
```

Now were set up to geocode stuff.  As an example, let’s geocode colleges and universities in Los Angeles using the data set *Colleges_and_Universities.csv*, which I uploaded onto GitHub.

```{r message = FALSE, warning = FALSE}
la.univ.df <- read_csv("https://raw.githubusercontent.com/crd150/data/master/LA_Colleges_and_Universities.csv")
```


The command `mutate_geocode()`  takes on two primary arguments: `data` and `location`.  The argument `data` is the data frame containing the addresses.  The argument `location` is a string variable containing your address information.  Address information at the minimum includes street prefix (e.g. East), number, name, suffix (eg. St., Ave.), city and state.  More information is better, so including a zipcode is ideal. Note that `location` takes in only one variable - this means that you don't separate out city, zipcode, and state from the street address - they all (address, city, state and zipcode) have to be one concatenated (string) line.  You can use the `unite()` function to concatenate fields. The addresses for the universities in *la.univ.df* is in one field and contains the prefix, house number, street name, city, state and zipcode. You can geocode addresses if they are missing one or more of these pieces of information - for example, you can geocode without the zipcode - but you may get some error.  You can also geocode intersections (e.g. 4th st and Main st) and landmarks ("University of California, Davis"). Check the file *Geocoding_Best_Practices* on Canvas for some tips on geocoding.

Unfortunately, Google Maps API can be a little finicky and sensitive when it comes to geocoding addresses.  In other words, although there are only 50 addresses to geocode in *la.univ.df*, you may get several addresses receiving the following warning.

````
`r ''`geocode failed with status OVER_QUERY_LIMIT, location = ...
````

This means that Google Maps thinks you've gone over a geocoding query limit.  That may be true because Google does limit geocoding queries to prevent a query overload.  You can deal with this by regeocoding the addresses that receive this warning.  And keep doing this until you've got all addresses geocoded.  You can set this up using a `while()` loop.  A `while()` loop repeats an action until some criteria is met.  First, geocode the 50 addresses using `mutate_geocode()`

```{r message=FALSE, warning=FALSE}
la.univ.df.geo1 <- mutate_geocode(la.univ.df, location=address)
```

Look at all those pretty red words...

If you ended up with no warnings, you're all good - all your addresses were successfully geocoded.  If you end up getting the warning of a geocoding failure because of an over query limit constraint or get a result at the end that looks something like this

````
`r ''`There were 16 warnings (use warnings() to see them)
````

run the following `while()` loop

```{r warning=FALSE, message=FALSE}
while(sum(is.na(la.univ.df.geo1$lat)) != 0){
  la.univ.df1 <- la.univ.df.geo1 %>%
                filter(is.na(lat) == TRUE) %>%
                select(OBJECTID:address)
  la.univ.df.geo2 <- mutate_geocode(la.univ.df1, location=address )
  la.univ.df.geo1 <- bind_rows(filter(la.univ.df.geo1, is.na(lat) == FALSE), la.univ.df.geo2)
}
```

Note that this process might take some time, but not more than 5-10 minutes.  You can imagine that geocoding many (over 10,000) street addresses will take quite some time.

Let's break down the above code to understand what it is doing.  The code inside the parentheses of the `while()` command tells R to keep running the code inside the brackets `while` the total number of NAs (non geocoded addresses) after geocoding is not equal to 0.  In other words, the while loop keeps running the code until all the addresses are geocoded.  The next line of code after `while()` 

````
`r ''`la.univ.df1 <- la.univ.df.geo1 %>%
        filter(is.na(lat) == TRUE) %>%
        select(OBJECTID:address)
````

saves the addresses with NA values from the prior geocoded data frame and keeps the original variables.  So, *la.univ.df1* contains addresses that were not successfully geocoded.  The next line of code 

````
`r ''`la.univ.df.geo2 <- mutate_geocode(la.univ.df1, location=address)
````

geocodes these addresses.  The last line of code 

````
`r ''`la.univ.df.geo1 <- bind_rows(filter(la.univ.df.geo1, is.na(lat) == FALSE), la.univ.df.geo2)
````

uses the `bind_rows()` function to append (add rows) *la.univ.df.geo2* to the addresses  successfully geocoded (no NA values) from the prior geocoded data frame *la.univ.df.geo1*.  

The `while()` loop then goes back to its criteria in the parentheses - if this second round of geocoding still yields NA values, it will run the code again until all addresses are successfully geocoded.

Note that the final outcome *la.univ.df.geo1* is not a spatial object. It needs to be converted to one if it is to be used in generating a map output.  Make the data frame into an **sf** point object by using the variables *lon* and *lat* in the function `st_as_sf()`. Remember to use the CRS of *la.city.tigris*.

```{r}
la.univ.sf <- st_as_sf(la.univ.df.geo1, coords = c("lon", "lat"), 
                       crs = st_crs(la.city.tigris))
```

Now, map the colleges and universities.

```{r}
tm_shape(la.city.tigris) +
  tm_polygons() +
tm_shape(la.univ.sf) +  
  tm_symbols(size = 0.25, col = "red")
```

Or interactively

```{r message = FALSE}
tmap_mode("view")
tm_shape(la.univ.sf) +  
  tm_symbols(size = 0.25, shape=3, col = "red")
```


The street addresses in *LA_Colleges_and_Universities.csv* are pretty clean and thus geocoding was relatively smooth.  Geocoding, however, is a messy process when you've got dirty addresses.  Check the file Geocoding_Best_Practices.pdf in the Week 6 folder on Canvas for some tips on geocoding.


***

Website created and maintained by [Noli Brazil](https://nbrazil.faculty.ucdavis.edu/)



  
  